{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c30145",
   "metadata": {},
   "source": [
    "# Transfer Learning in PyTorch\n",
    "Written by Calden Wloka for CS 153\n",
    "\n",
    "This notebook draws heavily on the [official PyTorch transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) by Sasank Chilamkurthy.\n",
    "\n",
    "Some other extremely useful documentation you may find useful:\n",
    "- [Saving and loading models](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)\n",
    "    - You often need an object to persist across training environments or instances. This allows you to work around XSEDE's timeout limitations, or run multiple experiments at different times with the same model.\n",
    "- [Torchvision object detection finetuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "    - This is a more advanced tutorial looking at fine tuning a model to a new dataset. In particular, it shows you how to set up a custom data loader.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec3ca1",
   "metadata": {},
   "source": [
    "The notebook is organized as follows:\n",
    "1. Library Setup - Imports and device initialization\n",
    "2. Data Handling - Setting up a data loader to feed training and validation data to our model\n",
    "3. Model Handling - Setting up our model to import pre-trained weights and reconfigure for our new task\n",
    "4. Training Setup - Create a training function to train our model\n",
    "5. Execution - Perform transfer learning and investigate the results\n",
    "6. Experiment Sandbox - Try a few extensions and alternative formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7c9db",
   "metadata": {},
   "source": [
    "## Library Setup\n",
    "\n",
    "We're using quite a large number of lirbaries for this one, including a whole bunch of `torch` libraries. As with other deep learning examples we've seen so far, we can execute our transfer learning with a CPU device, but it is much more efficient on a GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17ceff",
   "metadata": {},
   "source": [
    "## Data Handling\n",
    "\n",
    "The first step in most deep learning tasks, and particularly any tasks that involve training or finetuning of models, is setting up your data handling functionality. This typically means creating a set of tools to iteratively feed training data to your model, but you also usually want to include the option of validation and/or test data.\n",
    "\n",
    "For this tutorial, we are going to train a classifier to differentiate between *ants* and *bees*. The dataset is available [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip).\n",
    "\n",
    "If you download and extract the dataset, you will notice that it is rather small; there are about 120 training images each for ants and bees (along with 75 validation images). It would be hard to train a classifier from scratch on this amount of data, but transfer learning can leverage pre-training from prior data.\n",
    "\n",
    "Furthermore, we will make use of some basic data augmentation for training in the form of randomized resizing and cropping (which introduces a form of spatial jitter), and randomized horizontal flipping. We also need to be sure to normalize our image to the same range of values that the network was initially trained over to maximally take advantage of the pre-trained features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17826680",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b25e5",
   "metadata": {},
   "source": [
    "We'll now set up our dataloader to perform file handling. This step involves interacting with the file structure under which you have the images stored. For our purposes, we have the following folder organization:\n",
    "\n",
    "         hymenoptera_data\n",
    "            ______|______\n",
    "            |           |\n",
    "          train        val\n",
    "         ___|___     ___|___\n",
    "         |     |     |     |\n",
    "       ants  bees   ants  bees\n",
    "       \n",
    "This allows the datahandler to know the type of data (training vs. validation) and class (ants vs. bees) based on its location, without needing to read from any annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'hymenoptera_data' # set this to your local path to the data root directory\n",
    "\n",
    "# here we build our list of images in each data category\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# The DataLoader class is a torch utility that is designed for easily iterating through a dataset\n",
    "# during training or testing.\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "# These lines are not critical, but are helpful for understanding our data and visualizating our results.\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c290b",
   "metadata": {},
   "source": [
    "Let's explore our data setup a little bit; make sure you understand what each part is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e06a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classes are:')\n",
    "print(class_names)\n",
    "print(' ')\n",
    "print('The amount of data is:')\n",
    "print(dataset_sizes)\n",
    "print(' ')\n",
    "print('The dataset dictionary is:')\n",
    "print(image_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8971d18",
   "metadata": {},
   "source": [
    "Another imporant part of data handling is to **_look at your data_**! Ideally, we want to look at our output *after* our dataloader has processed it, as that can help catch inappropriate augmentations or other transformation or data handling errors.\n",
    "\n",
    "To look at data after the dataloader has processed it, we need to grab the tensors from the device and put them back in our regular workspace, and reshape them into the standard shape that we expect images to take. Our `tensor_show` function assumes the images have been pulled into our workspace, but takes care of the reshaping. We will use a `torchvision.utils` function called `make_grid` to turn a batch of images into one long image, and later we will use this function to visualize our predictions and explicitly send our images to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e73fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_show(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "tensor_show(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5577581",
   "metadata": {},
   "source": [
    "## Model Handling\n",
    "\n",
    "Now that we have our data set up and ready for processing, we need a model to do that processing.\n",
    "\n",
    "For this demonstration, we are going to work with the pretrained ResNet18 model. Note that the [original tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) demonstrates two different transfer learning protocols: finetuning the whole network (i.e. allowing weights throughout the network to change), and treating the network as a fixed feature extractor (i.e. only training the final classifier layer; sometimes this may be extended to multiple fully connected \"readout\" layers).\n",
    "\n",
    "For this demo we will focus on the latter style, but both options can be useful.\n",
    "We do this by setting the `requires_grad` parameter of the model feature layers to `False`, thereby preventing the gradient from being computed over them and leaving them open to updates by the training routine. Since newly constructed modules have `requires_grad=True` by default, when we declare our new output layer, it will be the only layer with a gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11838de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We grab the whole pretrained model to start\n",
    "model_tl = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "for param in model_tl.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "# We declare a new fully connected layer that has the same input dimensions as the original\n",
    "# and now has the output dimensions of the number of classes in our target dataset\n",
    "num_ftrs = model_tl.fc.in_features\n",
    "model_tl.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# this command sends the model to our device\n",
    "model_tl = model_tl.to(device)\n",
    "\n",
    "# here we set what type of loss we plan to use. Since this is a recognition task,\n",
    "# cross entropy is a good loss function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# we also need to set up an optimizer. Our standard SGD works fine.\n",
    "optimizer_tl = optim.SGD(model_tl.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_tl, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a20204",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "So now that we have our dataset ready, and our model ready, it is time to define how we want that model to train using the data. This is typically done through an encapsulated training function which we will call `train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # We are going to run for a set number of epochs, but that doesn't mean our final epoch is our best.\n",
    "    # Keep track of which version of the model worked the best.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # data logging\n",
    "    losslog = [[],[]]\n",
    "    acclog = [[],[]]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # data logging\n",
    "            if phase == 'train':\n",
    "                losslog[0].append(epoch_loss)\n",
    "                acclog[0].append(epoch_acc.to('cpu'))\n",
    "            else:\n",
    "                losslog[1].append(epoch_loss)\n",
    "                acclog[1].append(epoch_acc.to('cpu'))\n",
    "\n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losslog, acclog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4806d",
   "metadata": {},
   "source": [
    "Just like looking at our data can be useful, it is also a very good idea to inspect your model predictions and not just rely on the validation accuracy. For that, we want a visualization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                tensor_show(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ea097",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Okay... after all that, it's time to run things! One of the nice things about deep learning is that, although the setup can be convoluted, once all your ducks are in a row you just kind of have to hit go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a714f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl, losslog, acclog = train_model(model_tl, criterion, optimizer_tl,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,26),losslog[0], 'r-')\n",
    "plt.plot(range(1,26),losslog[1], 'b-')\n",
    "plt.title('Loss')\n",
    "plt.legend(['training', 'validation'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1,26),acclog[0], 'r-')\n",
    "plt.plot(range(1,26),acclog[1], 'b-')\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['training', 'validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2539a0",
   "metadata": {},
   "source": [
    "## Experiment Sandbox\n",
    "\n",
    "So we saw that our initial setup isn't actually doing too much (the hymenoptera data is taken from ImageNet, so it's already data that our model is quite familiar with, so we seem to converge on decent performance _very_ fast). Depending on our application, maybe we need to try and squeeze out a bit more performance, in which case we can think of potential ways we might do that, perhaps by adjusting our data handling.\n",
    "\n",
    "Alternatively, maybe we want to try and apply this to data that is a little more different, like [bears](https://www.kaggle.com/datasets/anirudhg15/bears-fastai-2021) or [cats and dogs](https://www.kaggle.com/datasets/alvarole/asirra-cats-vs-dogs-object-detection-dataset?resource=download).\n",
    "\n",
    "Another option would be to explore transfer learning on a different network (e.g. VGG-16).\n",
    "\n",
    "Finally, we could try manipulating our network architecture more than simply swapping out the fully connected layers; what happens if we instead try and learn from only a sub-portion of the feature layers from ResNet18?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
